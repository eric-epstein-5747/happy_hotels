{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOAL: \n",
    "### 1. Identify topics amongs hotel reviews\n",
    "### 2. For each hotel and each topic assign that hotel a score for that topic\n",
    "### 3. Surface for each topic the most relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/eric/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Import relevant modules\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# re\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import our data\n",
    "happy = pd.read_csv(\"/media/eric/nachmanides/insight_data_science/resources/data_challenge/happy_hotels/hotel_happy_reviews.csv\")\n",
    "unhappy = pd.read_csv(\"/media/eric/nachmanides/insight_data_science/resources/data_challenge/happy_hotels/hotel_not_happy_reviews.csv\")\n",
    "all_reviews = pd.concat([happy,unhappy],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenize each sentence into a list of words, removing punctuation and unnecessary characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stayed', 'here', 'with', 'husband', 'and', 'sons', 'on', 'the', 'way', 'to', 'an', 'alaska', 'cruise', 'we', 'all', 'loved', 'the', 'hotel', 'great', 'experience', 'ask', 'for', 'room', 'on', 'the', 'north', 'tower', 'facing', 'north', 'west', 'for', 'the', 'best', 'views', 'we', 'had', 'high', 'floor', 'with', 'stunning', 'view', 'of', 'the', 'needle', 'the', 'city', 'and', 'even', 'the', 'cruise', 'ships', 'we', 'ordered', 'room', 'service', 'for', 'dinner', 'so', 'we', 'could', 'enjoy', 'the', 'perfect', 'views', 'room', 'service', 'dinners', 'were', 'delicious', 'too', 'you', 'are', 'in', 'perfect', 'spot', 'to', 'walk', 'everywhere', 'so', 'enjoy', 'the', 'city', 'almost', 'forgot', 'heavenly', 'beds', 'were', 'heavenly', 'too']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuation\n",
    "\n",
    "data_words = list(sent_to_words(all_reviews.Description))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stayed', 'here', 'with', 'husband', 'and', 'sons', 'on', 'the', 'way', 'to', 'an', 'alaska_cruise', 'we', 'all', 'loved', 'the', 'hotel', 'great', 'experience', 'ask', 'for', 'room', 'on', 'the', 'north', 'tower', 'facing', 'north', 'west', 'for', 'the', 'best', 'views', 'we', 'had', 'high', 'floor', 'with', 'stunning', 'view', 'of', 'the', 'needle', 'the', 'city', 'and', 'even', 'the', 'cruise_ships', 'we', 'ordered', 'room', 'service', 'for', 'dinner', 'so', 'we', 'could', 'enjoy', 'the', 'perfect', 'views', 'room', 'service', 'dinners', 'were', 'delicious', 'too', 'you', 'are', 'in', 'perfect', 'spot', 'to', 'walk', 'everywhere', 'so', 'enjoy', 'the', 'city', 'almost', 'forgot', 'heavenly', 'beds', 'were', 'heavenly', 'too']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stay', 'husband', 'son', 'alaska_cruise', 'love', 'hotel', 'great', 'experience', 'ask', 'face', 'good', 'view', 'high', 'floor', 'stunning', 'view', 'city', 'even', 'order', 'room', 'service', 'dinner', 'could', 'enjoy', 'perfect', 'view', 'room', 'service', 'dinner', 'delicious', 'perfect', 'spot', 'walk', 'everywhere', 'enjoy', 'city', 'almost', 'forget', 'heavenly', 'bed', 'heavenly']]\n"
     ]
    }
   ],
   "source": [
    "# Call the functions in order\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 2), (25, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them.\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('alaska_cruise', 1),\n",
       "  ('almost', 1),\n",
       "  ('ask', 1),\n",
       "  ('bed', 1),\n",
       "  ('city', 2),\n",
       "  ('could', 1),\n",
       "  ('delicious', 1),\n",
       "  ('dinner', 2),\n",
       "  ('enjoy', 2),\n",
       "  ('even', 1),\n",
       "  ('everywhere', 1),\n",
       "  ('experience', 1),\n",
       "  ('face', 1),\n",
       "  ('floor', 1),\n",
       "  ('forget', 1),\n",
       "  ('good', 1),\n",
       "  ('great', 1),\n",
       "  ('heavenly', 2),\n",
       "  ('high', 1),\n",
       "  ('hotel', 1),\n",
       "  ('husband', 1),\n",
       "  ('love', 1),\n",
       "  ('order', 1),\n",
       "  ('perfect', 2),\n",
       "  ('room', 2),\n",
       "  ('service', 2),\n",
       "  ('son', 1),\n",
       "  ('spot', 1),\n",
       "  ('stay', 1),\n",
       "  ('stunning', 1),\n",
       "  ('view', 3),\n",
       "  ('walk', 1)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find the optimal number of topics for LDA\n",
    "#### Build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value. Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a function that  trains multiple LDA models and provides the models and \n",
    "### their corresponding coherence scores.\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=42,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f5cc854759e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Execute the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_coherence_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-5d8097759672>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[0;34m(dictionary, corpus, texts, limit, start, step)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                            \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                            per_word_topics=True)\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mcoherencemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c_v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    992\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reached the end of input; now waiting for all remaining jobs to finish\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m                         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0mother\u001b[0m  \u001b[0;31m# frees up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[0;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# update self with the new blend; also keep track of how much did\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# the topics change through this update, to assess convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mprevious_Elogbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mget_Elogbeta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mPosterior\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Execute the function\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph coherence scores as a function of number of topics\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model that gave the highest coherence value before flattening out, and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. View the topics in the best LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.107*\"tiny\" + 0.081*\"level\" + 0.060*\"lounge\" + 0.051*\"one\" + 0.048*\"valet\" '\n",
      "  '+ 0.048*\"certainly\" + 0.047*\"pull\" + 0.046*\"garage\" + 0.041*\"step\" + '\n",
      "  '0.035*\"hall\"'),\n",
      " (1,\n",
      "  '0.097*\"dark\" + 0.073*\"bedroom\" + 0.065*\"foot\" + 0.056*\"clearly\" + '\n",
      "  '0.055*\"replace\" + 0.053*\"unit\" + 0.045*\"curtain\" + 0.044*\"outdate\" + '\n",
      "  '0.043*\"dining\" + 0.042*\"concern\"'),\n",
      " (2,\n",
      "  '0.070*\"month\" + 0.068*\"uncomfortable\" + 0.055*\"arrival\" + 0.051*\"decor\" + '\n",
      "  '0.049*\"literally\" + 0.043*\"driver\" + 0.042*\"soon\" + 0.042*\"total\" + '\n",
      "  '0.040*\"thought\" + 0.039*\"manage\"'),\n",
      " (3,\n",
      "  '0.125*\"year\" + 0.096*\"always\" + 0.058*\"send\" + 0.057*\"home\" + 0.050*\"name\" '\n",
      "  '+ 0.047*\"hope\" + 0.045*\"write\" + 0.043*\"wonderful\" + 0.043*\"love\" + '\n",
      "  '0.033*\"resort\"'),\n",
      " (4,\n",
      "  '0.076*\"bar\" + 0.073*\"wait\" + 0.043*\"move\" + 0.039*\"probably\" + '\n",
      "  '0.037*\"weekend\" + 0.036*\"deal\" + 0.032*\"standard\" + 0.032*\"extra\" + '\n",
      "  '0.030*\"low\" + 0.029*\"access\"'),\n",
      " (5,\n",
      "  '0.282*\"stay\" + 0.126*\"staff\" + 0.109*\"clean\" + 0.097*\"location\" + '\n",
      "  '0.086*\"great\" + 0.045*\"friendly\" + 0.035*\"comfortable\" + 0.029*\"helpful\" + '\n",
      "  '0.019*\"visit\" + 0.015*\"city\"'),\n",
      " (6,\n",
      "  '0.151*\"complaint\" + 0.147*\"website\" + 0.147*\"value\" + 0.106*\"group\" + '\n",
      "  '0.101*\"center\" + 0.058*\"advance\" + 0.040*\"wireless\" + 0.035*\"warn\" + '\n",
      "  '0.032*\"tax\" + 0.030*\"housekeeper\"'),\n",
      " (7,\n",
      "  '0.514*\"desk\" + 0.368*\"front\" + 0.019*\"though\" + 0.015*\"lighting\" + '\n",
      "  '0.014*\"function\" + 0.009*\"impressed\" + 0.007*\"firm\" + 0.005*\"concert\" + '\n",
      "  '0.000*\"dirty\" + 0.000*\"finally\"'),\n",
      " (8,\n",
      "  '0.160*\"tired\" + 0.148*\"lady\" + 0.148*\"hand\" + 0.137*\"difficult\" + '\n",
      "  '0.093*\"afternoon\" + 0.051*\"boyfriend\" + 0.046*\"ideal\" + 0.021*\"bridge\" + '\n",
      "  '0.000*\"dirty\" + 0.000*\"finally\"'),\n",
      " (9,\n",
      "  '0.190*\"window\" + 0.152*\"high\" + 0.133*\"change\" + 0.060*\"cool\" + '\n",
      "  '0.054*\"hair\" + 0.048*\"th\" + 0.035*\"mold\" + 0.032*\"soap\" + '\n",
      "  '0.031*\"maintenance\" + 0.031*\"lift\"'),\n",
      " (10,\n",
      "  '0.070*\"room\" + 0.051*\"hotel\" + 0.019*\"would\" + 0.019*\"night\" + 0.017*\"get\" '\n",
      "  '+ 0.015*\"go\" + 0.013*\"bed\" + 0.012*\"time\" + 0.012*\"check\" + 0.011*\"good\"'),\n",
      " (11,\n",
      "  '0.101*\"bottle\" + 0.083*\"computer\" + 0.078*\"follow\" + 0.069*\"typical\" + '\n",
      "  '0.069*\"black\" + 0.061*\"gym\" + 0.051*\"somewhere\" + 0.049*\"stick\" + '\n",
      "  '0.049*\"nee\" + 0.048*\"print\"'),\n",
      " (12,\n",
      "  '0.211*\"coffee\" + 0.069*\"glass\" + 0.059*\"complimentary\" + 0.054*\"enter\" + '\n",
      "  '0.051*\"continental\" + 0.048*\"tea\" + 0.046*\"wine\" + 0.035*\"throw\" + '\n",
      "  '0.033*\"machine\" + 0.032*\"egg\"'),\n",
      " (13,\n",
      "  '0.102*\"toilet\" + 0.090*\"management\" + 0.074*\"wifi\" + 0.072*\"sound\" + '\n",
      "  '0.067*\"unfortunately\" + 0.047*\"answer\" + 0.046*\"email\" + 0.046*\"question\" + '\n",
      "  '0.041*\"share\" + 0.036*\"advertise\"'),\n",
      " (14,\n",
      "  '0.117*\"carpet\" + 0.063*\"instead\" + 0.058*\"terrible\" + 0.054*\"wear\" + '\n",
      "  '0.050*\"update\" + 0.048*\"renovation\" + 0.036*\"style\" + 0.030*\"fan\" + '\n",
      "  '0.025*\"idea\" + 0.025*\"matter\"'),\n",
      " (15,\n",
      "  '0.269*\"property\" + 0.118*\"date\" + 0.098*\"suppose\" + 0.089*\"fee\" + '\n",
      "  '0.068*\"suggest\" + 0.036*\"white\" + 0.033*\"fruit\" + 0.032*\"juice\" + '\n",
      "  '0.032*\"selection\" + 0.024*\"wallpaper\"'),\n",
      " (16,\n",
      "  '0.145*\"husband\" + 0.098*\"wrong\" + 0.087*\"bill\" + 0.080*\"watch\" + '\n",
      "  '0.068*\"forget\" + 0.065*\"lock\" + 0.055*\"everywhere\" + 0.049*\"age\" + '\n",
      "  '0.046*\"central\" + 0.045*\"young\"'),\n",
      " (17,\n",
      "  '0.302*\"smell\" + 0.180*\"sheet\" + 0.094*\"renovate\" + 0.058*\"temperature\" + '\n",
      "  '0.055*\"hit\" + 0.050*\"post\" + 0.040*\"heater\" + 0.035*\"assume\" + '\n",
      "  '0.033*\"certain\" + 0.028*\"select\"'),\n",
      " (18,\n",
      "  '0.054*\"breakfast\" + 0.047*\"area\" + 0.044*\"walk\" + 0.038*\"parking\" + '\n",
      "  '0.035*\"restaurant\" + 0.035*\"free\" + 0.035*\"minute\" + 0.032*\"close\" + '\n",
      "  '0.029*\"pool\" + 0.024*\"car\"'),\n",
      " (19,\n",
      "  '0.153*\"service\" + 0.064*\"experience\" + 0.050*\"review\" + 0.049*\"business\" + '\n",
      "  '0.044*\"return\" + 0.042*\"trip\" + 0.034*\"ever\" + 0.032*\"customer\" + '\n",
      "  '0.027*\"due\" + 0.022*\"extremely\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(optimal_model.print_topics())\n",
    "doc_lda = optimal_model[corpus]\n",
    "\n",
    "\n",
    "# NOTES: \n",
    "# The weights reflect how important a keyword is to that topic.\n",
    "# Looking at these keywords, we can guess what each topic is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -10.722577140998212\n",
      "\n",
      "Coherence Score:  0.26429871586107045\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', optimal_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=optimal_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTES: \n",
    "##### 1. Each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic.\n",
    "##### 2. A good topic model will have fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant.\n",
    "##### 3. A model with too many topics, will typically have many overlaps, small sized bubbles clustered in one region of the chart.\n",
    "##### 4. If you move the cursor over one of the bubbles, the words and bars on the right-hand side will update. These words are the salient keywords that form the selected topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
